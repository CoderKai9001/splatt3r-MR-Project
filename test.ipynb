{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad04370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import MAST3RGaussians\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as tfm\n",
    "from typing import Tuple, Union, Optional\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "sys.path.append('src/mast3r_src')\n",
    "sys.path.append('src/mast3r_src/dust3r')\n",
    "sys.path.append('src/pixelsplat_src')\n",
    "\n",
    "from dust3r.utils.image import load_images\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.utils.misc import hash_md5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "H,W = 512,512\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Applies pre-processing transformations to the image. \n",
    "    \"\"\"\n",
    "    _, h, w = img.shape\n",
    "    orig_shape = h, w\n",
    "\n",
    "    # Normalize the image\n",
    "    normalize = tfm.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    img = normalize(img).unsqueeze(0)\n",
    "\n",
    "    return img, orig_shape\n",
    "\n",
    "def load_single_image(path: Union[str, Path], resize: Optional[Union[int, Tuple]] = None, rot_angle: float = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loads a single image and resizes it to the pairs = make_pairs(imgs, scene_graph=scene_graph, prefilter=None, symmetrize=True) Height and Width specified in the config.\n",
    "    \"\"\"\n",
    "    if isinstance(resize, int):\n",
    "        resize = (resize, resize)\n",
    "    if isinstance(path, str):\n",
    "        path = Path(path)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    else:\n",
    "        img = path\n",
    "    img = tfm.ToTensor()(img)\n",
    "    if resize is not None:\n",
    "        img = tfm.Resize(resize, antialias=True)(img)\n",
    "    img = tfm.functional.rotate(img, rot_angle)\n",
    "    return img\n",
    "\n",
    "# def load_images(img0_path, img1_path):\n",
    "#     \"\"\"\n",
    "#     Loads and calls pre-processing to get the images ready for mast3r inference\n",
    "#     \"\"\"\n",
    "#     img0 = load_single_image(img0_path, (H, W))\n",
    "#     img1 = load_single_image(img1_path, (H, W))\n",
    "\n",
    "#     img0, img0_orig_shape = preprocess_image(img0)\n",
    "#     img1, img1_orig_shape = preprocess_image(img1)\n",
    "\n",
    "#     img_pair = [\n",
    "#         {\"img\": img0, \"idx\": 0, \"instance\": 0, \"true_shape\": torch.tensor(img0.shape[-2:], dtype=torch.int32)},\n",
    "#         {\"img\": img1, \"idx\": 1, \"instance\": 1, \"true_shape\": torch.tensor(img1.shape[-2:], dtype=torch.int32)},\n",
    "#     ]\n",
    "\n",
    "#     return img_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd905294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "image_dir = Path(\"images\")\n",
    "output_dir = Path(\"pointclouds\")\n",
    "top = 2\n",
    "image_list = natsorted(os.listdir(f'{image_dir}'))[:top]\n",
    "image_list = [f'{image_dir}/{imgName}' for imgName in image_list]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "splatt3r_device=torch.device('cuda:1')\n",
    "image_list, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"brandonsmart/splatt3r_v1.0\"\n",
    "filename = \"epoch=19-step=1200.ckpt\"\n",
    "weights_path = hf_hub_download(repo_id=model_name, filename=filename)\n",
    "splatt3r_model = MAST3RGaussians.load_from_checkpoint(weights_path, map_location=splatt3r_device)\n",
    "splatt3r_model.to(splatt3r_device)  # Ensure model is on correct device\n",
    "print(f\"Successfully loaded Splatt3r model onto {splatt3r_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38135e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mast3r_model = AsymmetricMASt3R.from_pretrained(Path(\"checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\")).to(device)\n",
    "print(\"Succesfully loaded Mast3r model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266757c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "silent = False\n",
    "imgs = load_images(image_list, size=H, verbose=not silent)\n",
    "splatt3r_imgs = load_images(image_list, size=H, verbose=not silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb79c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4aae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in imgs:\n",
    "    img['img'] = img['img'].to(device)\n",
    "    img['original_img'] = img['original_img'].to(device)\n",
    "    img['true_shape'] = torch.from_numpy(img['true_shape'])\n",
    "\n",
    "for img in splatt3r_imgs:\n",
    "    img['img'] = img['img'].to(splatt3r_device)\n",
    "    img['original_img'] = img['original_img'].to(splatt3r_device)\n",
    "    img['true_shape'] = torch.from_numpy(img['true_shape'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77384b05",
   "metadata": {},
   "source": [
    "### Mast3r encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_feat1, m_feat2, m_pos1, m_pos2 = mast3r_model._encode_image_pairs(imgs[0]['img'], imgs[1]['img'], imgs[0]['true_shape'], imgs[1]['true_shape'])\n",
    "with torch.inference_mode():\n",
    "    (m_shape1, m_shape2), (m_feat1, m_feat2), (m_pos1, m_pos2) = mast3r_model._encode_symmetrized(imgs[0], imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09427f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_feat1.shape, m_feat2.shape, m_pos1.shape, m_pos2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7edb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd287556",
   "metadata": {},
   "source": [
    "### Splatt3r encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    (shape1, shape2), (feat1, feat2), (pos1, pos2) = splatt3r_model.encoder._encode_symmetrized(splatt3r_imgs[0], splatt3r_imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3158d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1, shape2, feat1.shape, feat2.shape, pos1.shape, pos2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07b572",
   "metadata": {},
   "source": [
    "### Mast3r decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    m_dec1, m_dec2 = mast3r_model._decoder(m_feat1, m_pos1, m_feat2, m_pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4994ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m_dec1), m_dec1[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbdd57",
   "metadata": {},
   "source": [
    "### Splatt3r decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63055630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    dec1, dec2 = splatt3r_model.encoder._decoder(feat1, pos1, feat2, pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dec1), dec1[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bc597",
   "metadata": {},
   "source": [
    "## Get preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39797ee8",
   "metadata": {},
   "source": [
    "### Splatt3r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41723579",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = splatt3r_model.encoder._downstream_head(1, [tok.float() for tok in dec1], shape1)\n",
    "pred_2 = splatt3r_model.encoder._downstream_head(2, [tok.float() for tok in dec2], shape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1199f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1['pts3d'].shape, pred_1['conf'].shape, pred_1['desc_conf'].shape, pred_1['scales'].shape, pred_1['rotations'].shape, pred_1['sh'].shape, pred_1['opacities'].shape, pred_1['means'].shape,    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb25d6",
   "metadata": {},
   "source": [
    "### Mast3r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b37c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_1 = mast3r_model._downstream_head(1, [tok.float() for tok in m_dec1], m_shape1)\n",
    "m_pred_2 = mast3r_model._downstream_head(2, [tok.float() for tok in m_dec2], m_shape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_1['pts3d'].shape, m_pred_1['conf'].shape, m_pred_1['desc_conf'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407fe45",
   "metadata": {},
   "source": [
    "### SparseGA exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444e0679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [on]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aditya.vadali/miniconda3/envs/splatt3r/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home2/aditya.vadali/miniconda3/envs/splatt3r/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home2/aditya.vadali/miniconda3/envs/splatt3r/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Successfully loaded Splatt3r model\n",
      ">> Loading a list of 4 images\n",
      " - adding images/1.jpg with resolution 1280x963 --> 512x512\n",
      " - adding images/2.jpg with resolution 1280x963 --> 512x512\n",
      " - adding images/3.jpg with resolution 1280x963 --> 512x512\n",
      " - adding images/4.jpg with resolution 1280x963 --> 512x512\n",
      " (Found 4 images)\n",
      "----Using scene graph method complete---\n",
      "12 pairs constructed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 15665.00it/s]\n",
      " 25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed canonical data for image: images/1.jpg and avg_gaussians keys: ['sh', 'scales', 'rotations', 'opacities', 'offsets', 'means']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:02<00:02,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed canonical data for image: images/2.jpg and avg_gaussians keys: ['sh', 'scales', 'rotations', 'opacities', 'offsets', 'means']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed canonical data for image: images/3.jpg and avg_gaussians keys: ['sh', 'scales', 'rotations', 'opacities', 'offsets', 'means']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed canonical data for image: images/4.jpg and avg_gaussians keys: ['sh', 'scales', 'rotations', 'opacities', 'offsets', 'means']\n",
      "[CANON_KEYS]['images/1.jpg', 'images/2.jpg', 'images/3.jpg', 'images/4.jpg']\n",
      "init focals = [463.9657  443.6589  456.11243 459.68405]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:11<00:00, 26.43it/s, lr=0.0000, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.14474214613437653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:15<00:00, 19.82it/s, lr=0.0000, loss=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.5878666043281555\n",
      "Final focals = [463.0467  463.86038 467.52655 466.56027]\n"
     ]
    }
   ],
   "source": [
    "from model_replacement_test import MASt3R\n",
    "\n",
    "gen3d = MASt3R(imgdir=Path(\"./images\"), outdir=Path(\"./pointclouds\"), sample_images=False)\n",
    "\n",
    "# run multi-view Mast3R SfM\n",
    "scene = gen3d.reconstruct_scene(outdir=str(gen3d.outdir),\n",
    "                                cache_dir=\"/scratch/mast3r_cache\",\n",
    "                                scene_graph=\"complete\",\n",
    "                                optim_level=\"refine+depth\",\n",
    "                                lr1=0.07, niter1=300,\n",
    "                                lr2=0.01, niter2=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a05e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.canonical_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf29ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.intrinsics[0], len(scene.intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.cam2w[0], len(scene.cam2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77083bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.depthmaps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d479bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.pts3d[0], len(scene.pts3d), scene.pts3d[0].shape #???? why is the shape like that? GPT says it's 64*64 (subsampled points) + mast3r correspondences = 6430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.pts3d_colors[0], scene.pts3d_colors[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.imgs[0][30, 40] # scene.imgs[img_id][px_y, px_x] gives the color of the pixel of that particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels, idxs, offsets = scene.anchors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f975f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels.shape, idxs.shape, offsets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f134cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48686d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_coords = pixels[:, :2].detach().cpu().numpy().astype(int)\n",
    "px_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.utils.device import to_numpy62144, 3\n",
    "pts3d, _, confs = to_numpy(scene.get_dense_pts3d(clean_depth=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b058dfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3804955"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(confs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323502f",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "\n",
    "anchors is a dictionary of the form:\n",
    "\n",
    "```python\n",
    "{'img_index': [pixels, idxs, offsets]}\n",
    "```\n",
    "\n",
    "- `pixels` is a list of homogenous pixel coordinates: `[[px_x, px_y, 1],....]`\n",
    "\n",
    "- `idxs` are the indexes of the closest anchor point for any point. (For anchor points, they are themselves.)\n",
    "\n",
    "- `offsets` how much the depth of the current pixel varies to that of the depth of the closest anchor point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: coords is your (N, 2) numpy array of [x, y] pixel positions\n",
    "# coords = np.array([...])\n",
    "\n",
    "def visualize_pixel_coords(coords):\n",
    "    xs = coords[:, 0]\n",
    "    ys = coords[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(xs, ys, s=1)  # s = point size\n",
    "    plt.gca().invert_yaxis()  # Flip Y to match image coordinate system (origin at top-left)\n",
    "    plt.xlim(0, 512)\n",
    "    plt.ylim(0, 512)\n",
    "    plt.title(\"Anchor Points Visualization\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "pixels = scene.anchors[3][0][:, :2].detach().cpu().numpy().astype(int)\n",
    "\n",
    "visualize_pixel_coords(pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1['pts3d'].shape, pred_1['scales'].shape, pred_1['rotations'].shape, pred_1['sh'].shape, pred_1['opacities'].shape, pred_1['means'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb991b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_hashes_list(scene):\n",
    "    image_hashes = [None]*len(scene.img_paths)\n",
    "    for i, img_path in enumerate(scene.img_paths):\n",
    "        img_hash = hash_md5(img_path)\n",
    "        image_hashes[i] = img_hash\n",
    "        print(f\"Image {i}: {img_path} -> {img_hash}\")\n",
    "    \n",
    "    return image_hashes\n",
    "\n",
    "def load_gaussian(scene, cache_dir, index):\n",
    "    image_hahses = get_image_hashes_list(scene)\n",
    "    return torch.load(f\"{cache_dir}/{image_hahses[index]}.pth\")\n",
    "\n",
    "# /home2/aditya.vadali/splatt3r-MR-Project/mast3r_cache/gaussian_attributes/0ad929c4b83d461d351cfe97d8cb7558.pth\n",
    "# gaussians = torch.load('mast3r_cache/gaussian_attributes/0ad929c4b83d461d351cfe97d8cb7558.pth')\n",
    "# (sh, scales, rotations, opacities, means) = gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sh, scales, rotations, opacities, means) = load_gaussian(scene, 'mast3r_cache/gaussian_attributes', 0)\n",
    "sh.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means.squeeze()[0][0], mapping[(0,0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64eb208",
   "metadata": {},
   "source": [
    "### Create (x,y,z) to {gaussians} mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels0, pixels1 = scene.anchors[0][0], scene.anchors[1][0]\n",
    "pts3d_img0 = scene.pts3d[0]    # 3D points for image 0\n",
    "pt_to_gaussian_map = {}\n",
    "\n",
    "for i, [x, y, s] in enumerate(pixels0):\n",
    "    # Use the 3D point from image 0 at index i\n",
    "    point_3d = pts3d_img0[i]\n",
    "    \n",
    "    # Get Gaussian parameters at pixel (x, y) from the full prediction\n",
    "    pt_to_gaussian_map[i] = {  # Use index i as key, or convert point_3d to tuple\n",
    "        'pixel': (int(x), int(y)),\n",
    "        'point_3d': point_3d,\n",
    "        'sh': sh.squeeze()[int(y), int(x)],\n",
    "        'scales': scales.squeeze()[int(y), int(x)],\n",
    "        'rotations': rotations.squeeze()[int(y), int(x)],\n",
    "        'opacities': opacities.squeeze()[int(y), int(x)],\n",
    "        'means': means.squeeze()[int(y), int(x)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b885dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mast3r.utils.misc import hash_md5\n",
    "\n",
    "# pixels = []\n",
    "# for i in range(len(scene.anchors)):\n",
    "#     pixels.append(scene.anchors[i][0])\n",
    "\n",
    "# Get MD5 hashes for each image in the scene\n",
    "image_hashes = []\n",
    "for i, img_path in enumerate(scene.img_paths):\n",
    "    img_hash = hash_md5(img_path)\n",
    "    image_hashes.append((i, img_path, img_hash))\n",
    "    print(f\"Image {i}: {img_path} -> {img_hash}\")\n",
    "\n",
    "# Access Gaussian attributes for each image\n",
    "cache_dir = \"mast3r_cache\"\n",
    "gaussian_attributes = {}\n",
    "\n",
    "for img_idx, img_path, img_hash in image_hashes:\n",
    "    gaussians_path = f\"{cache_dir}/gaussian_attributes/{img_hash}.pth\"\n",
    "    \n",
    "    try:\n",
    "        # Load the Gaussian attributes for this image\n",
    "        gaussians = torch.load(gaussians_path)\n",
    "        sh, scales, rotations, opacities, means = gaussians\n",
    "        \n",
    "        gaussian_attributes[img_idx] = {\n",
    "            'image_path': img_path,\n",
    "            'hash': img_hash,\n",
    "            'sh': sh,\n",
    "            'scales': scales, \n",
    "            'rotations': rotations,\n",
    "            'opacities': opacities,\n",
    "            'means': means\n",
    "        }\n",
    "        \n",
    "        print(f\"Loaded Gaussians for image {img_idx}: {sh.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Gaussian attributes not found for image {img_idx} (hash: {img_hash})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_attributes[0]['sh'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.geometry as geometry\n",
    "\n",
    "# colors = scene.pts3d_colors\n",
    "covariances = geometry.build_covariance(scales, rotations)\n",
    "\n",
    "coords_to_gaussians_map = {}\n",
    "\n",
    "def map_dense_pts3d_to_pixels_with_colors(scene):\n",
    "    pts3d_dense, _, _ = scene.get_dense_pts3d(clean_depth=True)\n",
    "    \n",
    "    mapping = {}\n",
    "    for img_idx, pts3d_img in enumerate(pts3d_dense):\n",
    "        img = scene.imgs[img_idx]\n",
    "        H, W = 512, 512\n",
    "        \n",
    "        for y in range(H):\n",
    "            for x in range(W):\n",
    "                linear_idx = y * W + x\n",
    "                if linear_idx < len(pts3d_img):\n",
    "                    pt_3d = pts3d_img[linear_idx]\n",
    "                    color = img[y, x] \n",
    "                    \n",
    "                    mapping[(img_idx, x, y)] = {\n",
    "                        'pt_3d': pt_3d,\n",
    "                        'color': color \n",
    "                    }\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "mapping = map_dense_pts3d_to_pixels_with_colors(scene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f07c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[(0,32,32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img_index, x, y), mapping_dict in mapping.items():\n",
    "    covariances = geometry.build_covariance(gaussian_attributes[img_index]['scales'], gaussian_attributes[img_index]['rotations'])\n",
    "    coords_to_gaussians_map[tuple(mapping_dict['pt_3d'].detach().cpu().numpy())] = {\n",
    "        'pixel': (int(x), int(y)),\n",
    "        'image_index': img_index,\n",
    "        'color': mapping_dict['color'],\n",
    "        'sh': gaussian_attributes[img_index]['sh'].squeeze()[int(y), int(x)],\n",
    "        'scales': gaussian_attributes[img_index]['scales'].squeeze()[int(y), int(x)],\n",
    "        'rotations': gaussian_attributes[img_index]['rotations'].squeeze()[int(y), int(x)],\n",
    "        'opacities': gaussian_attributes[img_index]['opacities'].squeeze()[int(y), int(x)],\n",
    "        'means': gaussian_attributes[img_index]['means'].squeeze()[int(y), int(x)],\n",
    "        'covariances': covariances.squeeze()[int(y), int(x)]# The decoder which we use to render the predicted Gaussians into\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(coords_to_gaussians_map.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40aebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from src.mast3r_src.dust3r.dust3r.viz import OPENGL, pts3d_to_trimesh, cat_meshes\n",
    "from plyfile import PlyData, PlyElement\n",
    "from scipy.spatial.transform import Rotation\n",
    "import einops\n",
    "\n",
    "def save_gaussians_as_ply(coords_to_gaussians_map, save_path):\n",
    "    \"\"\"Save Gaussians as PLY file using spherical harmonics for color\"\"\"\n",
    "    \n",
    "    def construct_list_of_attributes(num_rest: int) -> list[str]:\n",
    "        '''Construct a list of attributes for the PLY file format'''\n",
    "        attributes = [\"x\", \"y\", \"z\", \"nx\", \"ny\", \"nz\"]\n",
    "        # Use spherical harmonics for color (first 3 coefficients = DC terms = RGB)\n",
    "        for i in range(3):\n",
    "            attributes.append(f\"f_dc_{i}\")\n",
    "        for i in range(num_rest):\n",
    "            attributes.append(f\"f_rest_{i}\")\n",
    "        attributes.append(\"opacity\")\n",
    "        for i in range(3):\n",
    "            attributes.append(f\"scale_{i}\")\n",
    "        for i in range(4):\n",
    "            attributes.append(f\"rot_{i}\")\n",
    "        # No explicit RGB fields - color comes from f_dc_0, f_dc_1, f_dc_2\n",
    "        return attributes\n",
    "\n",
    "    def covariance_to_quaternion_and_scale(covariances):\n",
    "        '''Convert the covariance matrix to quaternion and scale'''\n",
    "        U, S, V = torch.linalg.svd(covariances)\n",
    "        scale = torch.sqrt(S).detach().cpu().numpy()\n",
    "        rotation_matrix = torch.bmm(U, V.transpose(-2, -1))\n",
    "        rotation_matrix_np = rotation_matrix.detach().cpu().numpy()\n",
    "        rotation = Rotation.from_matrix(rotation_matrix_np)\n",
    "        quaternion = rotation.as_quat()\n",
    "        return quaternion, scale\n",
    "\n",
    "    def rgb_to_sh0(rgb):\n",
    "        \"\"\"Convert RGB color to spherical harmonic DC coefficient\"\"\"\n",
    "        # SH DC coefficient is C0 = 1/(2*sqrt(pi))\n",
    "        C0 = 0.28209479177387814\n",
    "        return (rgb - 0.5) / C0\n",
    "\n",
    "    # Collect the Gaussian parameters from the map\n",
    "    means_list = []\n",
    "    covariances_list = []\n",
    "    harmonics_list = []\n",
    "    opacities_list = []\n",
    "    \n",
    "    for pt_3d_tuple, gaussian_data in coords_to_gaussians_map.items():\n",
    "        means_list.append(list(pt_3d_tuple))\n",
    "        covariances_list.append(gaussian_data['covariances'].detach().cpu())\n",
    "        \n",
    "        # Use the original SH coefficients OR convert RGB to SH\n",
    "        # if 'sh' in gaussian_data:\n",
    "        #     # Use the predicted spherical harmonics as-is (original colors)\n",
    "        #     sh_coeffs = gaussian_data['sh'].detach().cpu()\n",
    "        #     harmonics_list.append(sh_coeffs)\n",
    "        # else:\n",
    "        # Convert RGB color to spherical harmonics DC coefficients\n",
    "        rgb_color = gaussian_data['color']  # Original color from scene\n",
    "        if torch.is_tensor(rgb_color):\n",
    "            rgb_color = rgb_color.detach().cpu()\n",
    "        else:\n",
    "            rgb_color = torch.tensor(rgb_color)\n",
    "        sh_dc = rgb_to_sh0(rgb_color)\n",
    "        harmonics_list.append(sh_dc)\n",
    "            \n",
    "        opacities_list.append(gaussian_data['opacities'].detach().cpu())\n",
    "    \n",
    "    if len(means_list) == 0:\n",
    "        print(\"ERROR: No Gaussians to save!\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    means = np.array(means_list)\n",
    "    covariances = torch.stack(covariances_list, dim=0)\n",
    "    harmonics = np.array(harmonics_list)\n",
    "    opacities = np.array(opacities_list).reshape(-1, 1)\n",
    "    \n",
    "    print(f\"Processing {len(means)} Gaussians\")\n",
    "    print(f\"SH DC coefficients (first 3 = RGB): {harmonics[0][:3]}\")\n",
    "    \n",
    "    # Convert covariances to quaternions and scales\n",
    "    rotations, scales = covariance_to_quaternion_and_scale(covariances)\n",
    "    \n",
    "    # Construct the attributes\n",
    "    rest = np.zeros_like(means)  # Normals\n",
    "    float_attrs = np.concatenate((means, rest, harmonics, opacities, np.log(scales), rotations), axis=-1)\n",
    "    \n",
    "    # Create dtype - ONLY float fields (no explicit RGB)\n",
    "    float_names = construct_list_of_attributes(0)  # num_rest=0 for now\n",
    "    dtype_full = [(name, \"f4\") for name in float_names]\n",
    "    \n",
    "    elements = np.empty(float_attrs.shape[0], dtype=dtype_full)\n",
    "    \n",
    "    # Fill the structured array\n",
    "    for i in range(float_attrs.shape[0]):\n",
    "        elements[i] = tuple(float_attrs[i])\n",
    "\n",
    "    # Save the point cloud\n",
    "    point_cloud = PlyElement.describe(elements, \"vertex\")\n",
    "    ply_data = PlyData([point_cloud])\n",
    "    ply_data.write(save_path)\n",
    "    \n",
    "    print(f\"Saved {len(elements)} Gaussians to {save_path}\")\n",
    "    print(\"Colors stored in spherical harmonics f_dc_0, f_dc_1, f_dc_2\")\n",
    "\n",
    "# Usage:\n",
    "save_gaussians_as_ply(coords_to_gaussians_map, \"pointclouds/final_maybe_clean_depth.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mast3r_src.demo import get_3D_model_from_scene\n",
    "\n",
    "model = get_3D_model_from_scene(outdir=\"pointclouds\", silent=False, min_conf_thr=1.5, \n",
    "                                as_pointcloud=True, clean_depth=True, transparent_cams=False, \n",
    "                                cam_size=0.2, TSDF_thresh=0.0, mask_sky=False, \n",
    "                                scene=scene )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab545e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(len(col), pt.shape) for pt, col in zip(scene.pts3d, scene.pts3d_colors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for pt, col in zip(scene.pts3d, scene.pts3d_colors):\n",
    "    for c in col:\n",
    "        print(c)\n",
    "        break\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ab383",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'max':pred_1['sh'].max(),\n",
    " 'min': pred_1['sh'].min()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max, r_min = max(scene.pts3d_colors[0][i][0] for i in range(len(scene.pts3d_colors[0][0]))), min(scene.pts3d_colors[0][i][0] for i in range(len(scene.pts3d_colors[0][0])))\n",
    "g_max, g_min = max(scene.pts3d_colors[0][i][1] for i in range(len(scene.pts3d_colors[0][0]))), min(scene.pts3d_colors[0][i][1] for i in range(len(scene.pts3d_colors[0][0])))\n",
    "b_max, b_min = max(scene.pts3d_colors[0][i][2] for i in range(len(scene.pts3d_colors[0][0]))), min(scene.pts3d_colors[0][i][2] for i in range(len(scene.pts3d_colors[0][0])))\n",
    "{'max': (r_max, g_max, b_max),\n",
    " 'min': (r_min, g_min, b_min)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af357220",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.pts3d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f29208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = scene.get_dense_pts3d(clean_depth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e712267",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img_index, x, y), pt in mapping.items():\n",
    "    print(img_index, x, y, pt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[(0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3240c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splatt3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
